---
title: "ActualData"
author: "Richard Arnold"
date: '2024-01-26'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
load("opar.Rda")
library(readxl)
library(dplyr)
library(tidyverse)
library(tidyr)
library(readxl)
library(ggplot2)
library(maps)
library(sf)
library(geojsonsf)
library(rmapshaper)
library(magick)

library(algae)
source("funcs.R")
source("maps.R")
```

Subsets of Tasman/Marlborough

```{r}
bbox.tasmar <- c(1570,5370,1725,5520)*1000
xlim.tasmar <- bbox.tasmar[c(1,3)]
ylim.tasmar <- bbox.tasmar[c(2,4)]

bbox.marlselect <- c(1645,5415,1720,5495)*1000
bbox.marlselect <- c(1645,5412,1725,5502)*1000
xlim.marlselect <- bbox.marlselect[c(1,3)]
ylim.marlselect <- bbox.marlselect[c(2,4)]
xlim.havelock <- c(1660,1675)*1000
ylim.havelock <- c(5425,5440)*1000
xlim.nydia <- c(1662,1677)*1000
ylim.nydia <- c(5437,5450)*1000
xlim.arapawa <- c(1695,1718)*1000
ylim.arapawa <- c(5432,5452)*1000

# Coastal polygons for mapping
bbox.marlselect.polygon <- st_polygon(list(cbind(bbox.marlselect[c(1,3,3,1,1)],
                                                 bbox.marlselect[c(2,2,4,4,2)])))
coast.marlselect <- st_crop(coast, bbox.marlselect.polygon)

bbox.tasmar.polygon <- st_polygon(list(cbind(bbox.tasmar[c(1,3,3,1,1)],
                                             bbox.tasmar[c(2,2,4,4,2)])))
coast.tasmar <- st_crop(coast, bbox.tasmar.polygon)

# polygons in model
habspoly_sf <- geojson_sf("data/HABS_exp_6_release_polygons.geojson")
habspoly_sf <- st_transform(habspoly_sf, crs=st_crs(coast))
marlpoly_sf <- geojson_sf("data/Entire_marlb_polygons_volume.geojson")
marlpoly_sf <- st_transform(marlpoly_sf, crs=st_crs(coast))

marlpoly_sf.trim <- ms_erase(marlpoly_sf, coast.marlselect) 

globalcrs <- 4326
marlcrs <- st_crs(marlpoly_sf.trim)
```

Plot examples

```{r}
plot(st_geometry(coast.marlselect), col="darkgreen", reset=FALSE, xlim=xlim.havelock, ylim=ylim.havelock,
     main="Original polygons")
plot(st_geometry(marlpoly_sf), col="yellow", add=TRUE)
plot(st_centroid(st_geometry(marlpoly_sf)), pch="+", col="red", add=TRUE)
```

```{r}
plot(st_geometry(coast.marlselect), col="darkgreen", reset=FALSE, xlim=xlim.havelock, ylim=ylim.havelock,
     main="Trimmed polygons")
plot(st_geometry(marlpoly_sf.trim), col="yellow", add=TRUE)
plot(st_centroid(st_geometry(marlpoly_sf.trim)), pch="+", col="red", add=TRUE)
```

Full region

```{r}
plot(st_geometry(coast.marlselect), col="darkgreen", reset=FALSE,
     main="Trimmed polygons")
plot(st_geometry(marlpoly_sf.trim), col="yellow", add=TRUE)
plot(st_centroid(st_geometry(marlpoly_sf.trim)), pch="+", col="red", add=TRUE)
```

Volumes by colour

```{r}
plot((marlpoly_sf.trim %>% mutate(vol=volume/1e9))["vol"], key.pos=4, reset=FALSE, 
     main="Polygon volumes and centroids")#, xlim=xlim.havelock, ylim=ylim.havelock)
plot(st_geometry(coast.marlselect), col="darkgreen", add=TRUE)
plot(st_centroid(st_geometry(marlpoly_sf.trim)), pch="+", col="white", add=TRUE)
```

```{r}
plot((marlpoly_sf.trim %>% mutate(depth=volume/st_area(marlpoly_sf.trim)))["depth"], key.pos=4, reset=FALSE, 
     main="Polygon mean depths and centroids")#, xlim=xlim.havelock, ylim=ylim.havelock)
plot(st_geometry(coast.marlselect), col="darkgreen", add=TRUE)
plot(st_centroid(st_geometry(marlpoly_sf.trim)), pch="+", col="white", add=TRUE)
```


From Romain:

The full dataset is named “All MSQP phyto data to 01.05.23”. 
Coordinates for the sampling sites are located in the “Raw_MSQP_Locations”.

Questions for Romain:

1. there are 306 locations -- all are GNNNA or PGNNNA (NNN=three numeric digits)(A=letter A-Q) 
   [we only want the PG and G ones]

1. location PG121 (Onepipi) has no coordinates (lon=0, lat=0) should be as follows?
          lon       lat       x       y
     174.3733 -41.13811 1715253 5445003

   from Romain: -41.138812 174.368901

1. 86 of the locations have the same NNN designation, just with P or PG, and the same location
   Of the others with the same NNN most seem to be the close in location and name, 
   but a few (PG121  Onepipi and G121 Nydia) are long way apart.
   Which locations do we actual want to use?  Should we only use SITETYPE="PHYTO" (i.e. remove
   "ESTAR" - and there are two with no SITETYPE: G114, G119 - I guess these should be "ESTAR"?)
   
   use all of them
   
1. 11 of the locations are not in any of the ocean polygons - some fall outside of the polygon
   tiling (e.g. Durville), but others are on land but close to the sea
   
   they are likely just outside
   
1. In the detection data, which REPORTED_NAME values should we use?  
  e.g. All beginnning with "Alexandrium" or just "Alexandriam catanella" and "Alexandrium pacificum"?
  it'd be good to understand how to filter this data set as a whole - this might need a zoom call perhaps
  
  
* use data for 2018 and later
* we should only use Alexandrium pacificum - (catanella should be replaced pacificum, but there shouldn't
  be any catanella from 2018 onwards) [don't use Alexandrium SPP]
* sites are available online
  https://mpi.maps.arcgis.com/apps/webappviewer/index.html?id=fbdacfcb945c46118ff105ee45142fbe

```{r}
list.files("data/confidential")
```

Locations

Note: corrected locations as follows in the file "Raw_MSQP_Locations-Corrected.csv"

 - PG121 - Onepipi - did not have lon/lat: corrected to same lon/lat as PG092 Onepipi
 - G062 - D'Urville - fully outside tiled region - no match: OK leave as non-matching - no change to coordinates
 - G086 - Head Nikau - close to shore - OK retain: match to closest polygon
 - G096 - Te Mahia - on land - and a long way in: clearly wrong!  On MPI map Te Mahia should be at x=1681390.0056, y=5436803.2623
          But label on MPI map is G108?  
          in our list though G108 is Port Hardy (which is on D'Urville Island...)
          However MPI list of Port Hardy is G108 too, so the MPI map label is wrong too.
          Need to fix these coordinates to x=1681390.0056, y=5436803.2623 lon=173.9709, lat=-41.21606
 - G105 - Black Rock - OK - this is at the head of an inlet - OK retain: match to closest polygon
 - G128 - Cook Strait 1 - too far out: remove
 - PG017 - Anakoha Bay - OK - at the end of an inlet - OK retain: match to G017 (174.095 -41.004 1692086 5460222)
 - PG062 - D'Urville - fully outside tiled region - no match: remove
 - PG086 - Head Nikau - close to shore - OK retain: match to closest polygon
 - PG111 - Kenepuru Head - inland: should be the same as G111 (1693614 5441549), and not as recorded (1689756 5448481)
           Need to fix these coordinates: match to G111 Kenepuru Head
 - PG128 - Robertson Point - to far out to sea - no match: remove
 - PG237 - Port Gore 3 - on land - clearly wrong - should be (1700423.1103 5454097.5219)
           Need to fix these coordinates - match to G237 (1705190 5453050)

Use all of these

```{r}
fname <- paste0("data/confidential/","Raw_MSQP_Locations-Corrected.csv")
habloc <- read.csv(fname)
# convert locations to sf object
habloc_sf <- st_as_sf(habloc %>% mutate(xp=x,yp=y), 
                      coords=c("xp","yp"),
                      crs=marlcrs,
                      agr="constant")
# identify which polygon each location falls into
habloc_sf <- habloc_sf %>% 
  mutate(intersection=as.integer(st_intersects(geometry, marlpoly_sf)),
         CLUSTER_ID=if_else(is.na(intersection),NA,marlpoly_sf$CLUSTER_ID[intersection])) %>%
  select(-intersection)
```

Also get locations from the MPI website - but these are confusing, so we won't use them

```{r}
fnamempi1 <- paste0("data/confidential/","Marine_biotoxin_sample_site_locations_-_View.csv")
fnamempi2 <- paste0("data/confidential/","Non-Commercial_Regular_Sites_for_the_Shellfish_Biotoxin_-_View.csv")
mpiloc1 <- read.csv(fnamempi1)
mpiloc2 <- read.csv(fnamempi2)
names(mpiloc1)[1:2] <- c("lon","lat")
any(mpiloc1$code %in% habloc$siteID) # yes
any(mpiloc2$code %in% habloc$siteID) # none
all(habloc$siteID %in% mpiloc1$code) # no
is.unique(mpiloc1$code) # FALSE
mpiloc1.u <- do.call(rbind, by(mpiloc1, mpiloc1$code, function(x) x[1,,drop=FALSE]))
is.unique(mpiloc1.u$code) # TRUE
habloc_sf <- merge(habloc_sf, mpiloc1.u, by.x="siteID", by.y="code", all.x=TRUE, suffix=c("",".mpi"))
nrow(habloc_sf) # 306

habloc <- habloc_sf %>% st_drop_geometry()
```

```{r}
habloc$dd <- sqrt((habloc$lon-habloc$lon.mpi)^2 + (habloc$lat-habloc$lat.mpi)^2)
hist(habloc$dd, breaks=50)
habloc[!is.na(habloc$dd) & habloc$dd>0.1,]
```


```{r}
#View(habloc)
nrow(habloc) # 306
View(habloc)

#is.unique(habloc$siteID) # TRUE
#table(habloc$SITETYPE,exclude=NULL) # "", "ESTAR" and "PHYTO"
#      ESTAR PHYTO 
#    2   198   106 
#table(habloc$SITETYPE, substring(habloc$siteID,1,1), exclude=NULL)
#          G   P
#          2   0
#  ESTAR 197   1
#  PHYTO   0 106

# are the P and G sites the same?
hp <- habloc%>%filter(substring(siteID,1,1)=="P")  
hg <- habloc%>%filter(substring(siteID,1,1)=="G")
hp$siteNumber <- substring(hp$siteID,3)
hg$siteNumber <- substring(hg$siteID,2)
is.unique(hp$siteNumber) # TRUE
is.unique(hg$siteNumber) # TRUE
hpg <- merge(hp,hg,all=TRUE,by="siteNumber",suffixes=c(".p",".g"))
hpg$dx <- hpg$x.p-hpg$x.g
hpg$dy <- hpg$y.p-hpg$y.g
hpg[1:2,]
idx <- !is.na(hpg$x.p) & !is.na(hpg$x.g)
nrow(hpg[idx,]) # 103
# many differences are zero
table(hpg[idx,]$lon.p-hpg[idx,]$lon.g) # 87 are zero out of 103
table(hpg[idx,]$lat.p-hpg[idx,]$lat.g) # 88 are zero out of 103
table(hpg$dx) # 86 are zero out of 103
table(hpg$dy) # 86 are zero out of 103
hpg$matched <- FALSE
hpg$matched[idx] <- hpg$dx[idx]==0 & hpg$dy[idx]==0
table(hpg$matched) # FALSE=117, TRUE=86
table(hpg$matched[idx]) # FALSE=17, TRUE=86
View(hpg[idx & !hpg$matched,])
# particular bad match is PG121 (Onepipi) and G121 (Nydia Bay Mouth)

range(habloc$lon) # min is zero
habloc[habloc$lon==0,] # Location PG121 has lon=0, lat=0: Onepipi
## lon=174.37325650  lat=-41.13810965
#onepipi <- data.frame(lon=174.37325650, lat=-41.13810965)
## lon=174.368901, lat=-41.138812
#onepipi <- data.frame(lon=174.368901, lat=-41.138812)
#onepipi.lonlat <- st_as_sf(onepipi,
#                           coords=c("lon","lat"),
#                           crs=globalcrs,
#                           agr="constant")
#onepipi.xy <- st_transform(onepipi.lonlat, marlcrs)
#xy <- unclass(onepipi.xy$geometry[[1]])
#onepipi$x <- xy[1]; onepipi$y <- xy[2]
#onepipi
#x=1714893.2113, y=5444907.4273
#ra_getxy(onepipi.xy) #1714886 5444930
```

```{r}
plot(coast.tasmar$geometry, col="dark green", reset=FALSE, xlim=xlim.tasmar, ylim=ylim.tasmar)
rect(xlim.tasmar[1], ylim.tasmar[1], xlim.tasmar[2], ylim.tasmar[2])
plot(st_geometry(habloc_sf), pch=16, col="red", add=TRUE)
#points(habloc$x, habloc$y, pch=16, col="red")
title("Collection points in Tasman/Malborough")
```

```{r}
plot(coast.marlselect$geometry, col="dark green", xlim=xlim.marlselect, ylim=ylim.marlselect)
rect(xlim.marlselect[1], ylim.marlselect[1], xlim.marlselect[2], ylim.marlselect[2])
plot(st_geometry(habloc_sf), pch=16, col="red", add=TRUE)
#points(habloc$x, habloc$y, pch=16, col="red")
title("Collection points in Malborough")
```

Plot collection points within polygons

```{r}
siteid <- "G009"
cid <- habloc_sf$CLUSTER_ID[habloc_sf$siteID==siteid]
plot(st_geometry(coast.marlselect), reset=FALSE, xlim=xlim.nydia, ylim=ylim.nydia, col="darkgreen",
     main=habloc_sf$site_name[habloc_sf$siteID==siteid])
plot(st_geometry(marlpoly_sf.trim %>% filter(CLUSTER_ID==cid)), col="yellow", add=TRUE)
plot(st_geometry(habloc_sf %>% filter(siteID==siteid)), pch=16, col="red", cex=1, add=TRUE)
axis(1); axis(2); box()
```

Which points are not matched to a polygon?

```{r}
plot(coast.marlselect$geometry, col="dark green", xlim=xlim.marlselect, ylim=ylim.marlselect)
rect(xlim.marlselect[1], ylim.marlselect[1], xlim.marlselect[2], ylim.marlselect[2])
plot(st_geometry(habloc_sf %>% filter(is.na(CLUSTER_ID))), pch=16, col="red", add=TRUE)
#points(habloc$x, habloc$y, pch=16, col="red")
title("Collection points in Malborough\nnot matched to an ocean polygon")
```

```{r}
idx <- sapply(st_within(habloc_sf, bbox.marlselect.polygon),length)==1
#View(habloc_sf %>% filter(is.na(CLUSTER_ID) & idx)) #%>% as.data.frame()
outofarea_sf <- habloc_sf %>% filter(is.na(CLUSTER_ID) & idx) 

# Find the closest polygon
closest <- list()
for(i in seq_len(nrow(outofarea_sf))){
    closest[[i]] <- marlpoly_sf.trim[which.min(st_distance(marlpoly_sf.trim, outofarea_sf[i,])),]
}
outofarea_sf$CLUSTER_ID_closest <- sapply(closest, function(x) x$CLUSTER_ID)
outofarea <- outofarea_sf %>% st_drop_geometry()

outofarea_sf
## 7 sites, 2 are repeats
```

Simple feature collection with 7 features and 17 fields
Geometry type: POINT
Dimension:     XY
Bounding box:  xmin: 1659699 ymin: 5416581 xmax: 1708427 ymax: 5469771
Projected CRS: NZGD2000 / New Zealand Transverse Mercator 2000
  siteID   X       site_name SITETYPE     lon     lat       x       y notes CLUSTER_ID  lon.mpi  lat.mpi F__OBJECTID  id
1   G062  76        Durville    ESTAR 173.709 -40.921 1659699 5469771               NA       NA       NA          NA  NA
2   G086 100      Head Nikau    ESTAR 173.896 -41.170 1675161 5441984               NA 173.8956 -41.1701         165  76
3   G105 119      Black Rock    ESTAR 174.047 -41.114 1687903 5448060               NA       NA       NA          NA  NA
4   G128 142   Cook Strait 1    ESTAR 174.297 -41.395 1708427 5416581               NA 174.2967 -41.3953         636 856
5  PG062  43        Durville    PHYTO 173.709 -40.921 1659699 5469771               NA       NA       NA          NA  NA
6  PG086  53      Head Nikau    PHYTO 173.896 -41.170 1675161 5441984               NA       NA       NA          NA  NA
7  PG128  76 Robertson Point    PHYTO 174.297 -41.395 1708427 5416581               NA       NA       NA          NA  NA
  sample_type          name                geometry CLUSTER_ID_closest
1        <NA>          <NA> POINT (1659699 5469771)                227
2          PP    Head Nikau POINT (1675161 5441984)                403
3        <NA>          <NA> POINT (1687903 5448060)                 46
4          SF Cook Strait 1 POINT (1708427 5416581)                334
5        <NA>          <NA> POINT (1659699 5469771)                227
6        <NA>          <NA> POINT (1675161 5441984)                403
7        <NA>          <NA> POINT (1708427 5416581)                334

```{r}
for(i in seq_len(nrow(outofarea_sf))) {
  sid <- outofarea_sf$siteID[i]
  sname <- outofarea_sf$site_name[i]
  cid <- outofarea_sf$CLUSTER_ID_closest[i]
  idx <- habloc$siteID==sid
  xy <- c(habloc$x[idx], habloc$y[idx])
  plot(st_geometry(coast.marlselect), reset=FALSE, col="darkgreen",
       xlim=xy[1]+c(-1,1)*8000,
       ylim=xy[2]+c(-1,1)*8000,
       main=paste0("Site ", sid, " Polygon ",cid, " (",sname,")"))
  plot(st_geometry(marlpoly_sf.trim %>% filter(CLUSTER_ID==cid)), col="yellow", add=TRUE)
  plot(st_geometry(outofarea_sf %>% filter(siteID==sid)), pch=16, col="red", cex=1, add=TRUE)
  axis(1); axis(2); box()
}
```

 - G062 - D'Urville - fully outside tiled region - no match: remove
 - G086 - Head Nikau - close to shore - OK retain
 - G105 - Black Rock - OK - this is at the head of an inlet - OK retain
 - G128 - Cook Strait 1 - too far out: remove
 - PG062 - D'Urville - fully outside tiled region - no match: remove
 - PG086 - Head Nikau - close to shore - OK retain
 - PG128 - Robertson Point - to far out to sea - no match: remove

```{r}
outofarea_retain_sf <- outofarea_sf %>% filter(siteID%in%c("G086","G105","PG086"))
outofarea_retain_sf
```


```{r}
plot(st_geometry(coast.marlselect), reset=FALSE, col="darkgreen",
     main="Matching out of area sites to nearest polygon")
plot(st_geometry(marlpoly_sf.trim %>% filter(CLUSTER_ID%in%outofarea_retain_sf$CLUSTER_ID_closest)), col="yellow", add=TRUE)
plot(st_geometry(outofarea_retain_sf), pch=16, col="red", cex=1, add=TRUE)
axis(1); axis(2); box()
```

```{r}
habloc_sf <- merge(habloc_sf, st_drop_geometry(outofarea_retain_sf)[,c("siteID","CLUSTER_ID_closest")], all.x=TRUE)
idx <- is.na(habloc_sf$CLUSTER_ID) & !is.na(habloc_sf$CLUSTER_ID_closest)
habloc_sf$CLUSTER_ID[idx] <- habloc_sf$CLUSTER_ID_closest[idx]
habloc_sf$CLUSTER_ID_closest <- NULL
habloc <- habloc_sf %>% st_drop_geometry()
#write.csv(habloc, file="data/habloc.csv", row.names=FALSE)
```

```{r}
habloc <- read.csv("data/habloc.csv")
habloc_sf <- st_as_sf(habloc %>% mutate(xp=x,yp=y), 
                      coords=c("xp","yp"),
                      crs=marlcrs,
                      agr="constant")
```

Detections

```{r, error=FALSE, message=FALSE, warning=FALSE}
fname <- paste0("data/confidential/","All MSQP phyto data to 01.05.23.xlsx")
habdat <- read_excel(fname) %>% as.data.frame()
#View(habdat)
nrow(habdat) # 292786
habdat <- habdat[habdat$Site_Code%in%habloc$siteID,]
nrow(habdat) # 101842
habdat[1:2,]
```

Assign each event to a polygon

```{r}
habdat <- merge(habdat, habloc[,c("siteID","site_name","CLUSTER_ID","x","y")], by.x="Site_Code", by.y="siteID", all.x=TRUE)
nrow(habdat) # 101842
habdat <- habdat[!is.na(habdat$CLUSTER_ID),]
nrow(habdat) # 88479
table(habdat$REPORTED_NAME)
alexnames <- c("Alexandrium spp.",
               "Alexandrium cf. pacificum","Alexandrium pacificum",
               "Alexandrium cf. catenella","Alexandrium catenella")
habdat.alex <- habdat[habdat$REPORTED_NAME%in%alexnames,]
nrow(habdat.alex) # 887
#table(habdat.alex$REPORTED_NAME)
```

    Alexandrium catenella Alexandrium cf. catenella Alexandrium cf. pacificum     Alexandrium pacificum          Alexandrium spp. 
                      260                         9                        32                       576                        10 

```{r}
events <- habdat %>% group_by(Site_Code,SITE_DESCRIPTION,SAMPLED_DATE) %>% summarise(ndetections=n()) %>% ungroup()
nrow(events) # 12824
table(events$ndetections)
events[1:10,]
```

```{r}
events.alex <- merge(events, 
                     habdat.alex[,c("Site_Code","SITE_DESCRIPTION","SAMPLED_DATE","REPORTED_NAME",
                                    "Data_Value","UNITS","x","y","site_name","CLUSTER_ID")],
                     by=c("Site_Code","SITE_DESCRIPTION","SAMPLED_DATE"),
                     all.x=TRUE)
nrow(events.alex) # 12830: more than 12824
```

Check that the site names match
```{r}
check <- habdat.alex %>% group_by(Site_Code,SITE_DESCRIPTION,site_name) %>% summarise(nrow=n()) %>% ungroup()
check$SITE_DESCRIPTION <- gsub(".Marine$","",check$SITE_DESCRIPTION)
all(tolower(check$SITE_DESCRIPTION)==tolower(check$site_name)) # TRUE: same data source
#nrow(check) # 119
#View(check) # yes they do match
```


```{r}
tt <- table(paste(habdat.alex$Site_Code,habdat.alex$SAMPLED_DATE))
nn <- names(tt[tt>1])
habdat.alex[paste(habdat.alex$Site_Code,habdat.alex$SAMPLED_DATE)%in%nn,]
events.alex[paste(events.alex$Site_Code,events.alex$SAMPLED_DATE)%in%nn,]
```
 G010 2019-02-26 14:00:00  G073 2019-02-26 15:30:00 PG091 2014-02-26 13:53:00 
                        2                         2                         5 

     Site_Code         SITE_DESCRIPTION        SAMPLED_DATE ndetections             REPORTED_NAME   Data_Value      UNITS       x       y
272        G010       Hallam Cove.Marine 2019-02-26 14:00:00          13 Alexandrium cf. pacificum         2700 CELL_LITRE 1669388 5461359
273        G010       Hallam Cove.Marine 2019-02-26 14:00:00          13          Alexandrium spp.          400 CELL_LITRE 1669388 5461359
4927       G073 Head Of Nydia Bay.Marine 2019-02-26 15:30:00           9     Alexandrium pacificum         4000 CELL_LITRE 1665852 5442296
4928       G073 Head Of Nydia Bay.Marine 2019-02-26 15:30:00           9          Alexandrium spp.          200 CELL_LITRE 1665852 5442296
12504     PG091                Tio Point 2014-02-26 13:53:00          17     Alexandrium catenella         1300 CELL_LITRE 1704754 5434402
12505     PG091                Tio Point 2014-02-26 13:53:00          17     Alexandrium catenella         2100 CELL_LITRE 1704754 5434402
12506     PG091                Tio Point 2014-02-26 13:53:00          17     Alexandrium catenella         1400 CELL_LITRE 1704754 5434402
12507     PG091                Tio Point 2014-02-26 13:53:00          17     Alexandrium catenella          300 CELL_LITRE 1704754 5434402
12508     PG091                Tio Point 2014-02-26 13:53:00          17     Alexandrium catenella Not Detected CELL_LITRE 1704754 5434402

2014 detections will be ignorable (use data from 2018 only)
G010 and G073 remove Alexandrium spp. cases.
Achieve this by taking the events with the maximum Data_Value

```{r}
idx <- paste(events.alex$Site_Code,events.alex$SAMPLED_DATE)%in%nn
events.alex[idx,]
d1 <- events.alex[idx,]
d0 <- events.alex[!idx,]

d1 <- d1 %>% 
  group_by(Site_Code) %>%
  filter(Data_Value==max(as.numeric(Data_Value),na.rm=TRUE)) %>%
  ungroup() 
d1

events.alex <- rbind(d0,d1)
nrow(events.alex) # 12824 - OK
table(events.alex$Data_Value)
events.alex$Value <- as.numeric(events.alex$Data_Value)
events.alex$Value[is.na(events.alex$Value)] <- 0
events.alex$site_name <- NULL; events.alex$x <- NULL; events.alex$y <- NULL; events.alex$CLUSTER_ID <- NULL
events.alex$SITE_DESCRIPTION <- NULL
events.alex$UNITS <- NULL
events.alex <- merge(events.alex, habloc_sf[,c("siteID","site_name","x","y","CLUSTER_ID")] %>% st_drop_geometry(), 
                     by.x="Site_Code", by.y="siteID", all.x=TRUE)
#table(events.alex$ndetections,exclude=NULL)
events.alex$REPORTED_NAME[is.na(events.alex$REPORTED_NAME)] <- "No detection"
table(events.alex$REPORTED_NAME,exclude=NULL)
events.alex$Date <- as.Date(substring(events.alex$SAMPLED_DATE,1,10), format="%Y-%m-%d")
events.alex$DateTime <- as.POSIXct(events.alex$SAMPLED_DATE)

# Convert dates to weeks: move the date back to the previous Monday
events.alex$mDate <- make.monday(events.alex$Date)
events.alex$mWeek <- date.as.week(events.alex$mDate) ### as.integer(events.alex$mDate+3)%/%7
events.alex$mDay <- as.numeric(format(events.alex$Date,"%u"))

#data.frame(format(events.alex$Date,"%Y-%m-%d, %a"), 
#          format(events.alex$mDate,"%Y-%m-%d, %a"),
#           events.alex$mWeek,
#           events.alex$mDay)[1:10,]

table(format(events.alex$Date, "%a"))
table(format(events.alex$mDate, "%a"))
#gsub("-.$","-1",format(events.alex$Date, "%Y-%m-%u")[1:20])
#gsub("-.$","w1",format(events.alex$Date, "%Y-%m-%u")[1:20])
```

```{r}
#write.csv(events.alex %>% st_drop_geometry(),file="data/eventsAlexandrium.csv",row.names=FALSE)
events.alex <- read.csv("data/eventsAlexandrium.csv")
events.alex$Date <- as.Date(events.alex$Date)
events.alex$mDate <- as.Date(events.alex$mDate)
events.alex$DateTime <- as.POSIXct(events.alex$SAMPLED_DATE)
nrow(events.alex)
View(events.alex)
```

Check for multiple events in the same week
```{r}
tt <- table(paste(events.alex$Site_Code,events.alex$mWeek))
table(tt[tt>1])
```

This happens a lot!

  2   5   6 
215   2   3 

Make a unique version - just take the first sample in each POLYGON in each week
```{r}
events.alex.week1 <- events.alex %>%
                      group_by(CLUSTER_ID,mWeek) %>%
                      filter(mDay==min(mDay)) %>%
                      ungroup()
#View(events.alex.week1)
is.unique(paste(events.alex$CLUSTER_ID, events.alex$mWeek)) # FALSE
is.unique(paste(events.alex.week1$CLUSTER_ID, events.alex.week1$mWeek)) # FALSE

tt <- table(paste(events.alex.week1$CLUSTER_ID,events.alex.week1$mWeek))
table(tt[tt>1])
```

A few cases of multiple samples on a single day: some of these are at distinct locations, some are at the same 
location

 2  5  6 
31  2  3 

```{r}
nn <- names(tt[tt>1])
events.alex.week1[paste(events.alex.week1$CLUSTER_ID,events.alex.week1$mWeek) %in% nn,]
```

Just take the first in the week, and if the times are equal (some times are missing and are set to 00:00:00)
then take the first row

```{r}
sum(format(events.alex$DateTime,"%H:%M:%S")=="00:00:00") # 16 cases there the time is missing
```

Final version - take the observation with the maximum non-missing value
```{r}
events.alex.week <- events.alex %>%
                      group_by(CLUSTER_ID,mWeek) %>%
                      filter(Value==max(Value,na.rm=TRUE)) %>%
                      filter(row_number()==1) %>%
                      ungroup()
#View(events.alex.week)
is.unique(paste(events.alex$CLUSTER_ID, events.alex$mWeek)) # FALSE
is.unique(paste(events.alex.week$CLUSTER_ID, events.alex.week$mWeek)) # TRUE
nrow(events.alex.week) # 12564
```



```{r}
#write.csv(events.alex.week %>% st_drop_geometry(),file="data/eventsAlexandriumWeek.csv",row.names=FALSE)
events.alex.week <- read.csv("data/eventsAlexandriumWeek.csv")
events.alex.week$Date <- as.Date(events.alex.week$Date)
events.alex.week$mDate <- as.Date(events.alex.week$mDate)
events.alex.week$DateTime <- as.POSIXct(events.alex.week$SAMPLED_DATE)
nrow(events.alex.week) # 12586
#View(events.alex.week)
```

```{r}
hist(events.alex$Date, breaks="years")
```

```{r}
diff(range(events.alex.week$mWeek)) # 573 weeks
range(events.alex.week$Date)  #"2012-04-30" "2023-04-26"
nrow(events.alex.week) # 12586
nrow(events.alex.week[events.alex.week$Date>=as.Date("2018-01-01"),]) # 6411
```

```{r}
plot(events.alex.week$Date, events.alex.week$Value, 
     xlab="Date", ylab="Cells/litre",
     main="Detections of Alexandrium")
```

Restrict to 1 Jan 2018 onwards
```{r}
events.alex.week.2018 <- events.alex.week %>% 
  filter(Date>=as.Date("2018-01-01"))
```


Find the largest intensity observation in each polygon over the entire period (from 1 Jan 2018)
```{r}
maxobs <- events.alex.week.2018 %>%
  group_by(CLUSTER_ID) %>% 
  summarise(maxValue1000=max(Value,na.rm=TRUE)/1000, nobs=n()) %>%
  ungroup()
maxobs_sf <- merge(marlpoly_sf.trim, maxobs, all.x=TRUE)
maxobs_sf$nobs[is.na(maxobs_sf$nobs)] <- 0
nrow(maxobs_sf) # 408
sum(is.na(maxobs_sf$maxValue1000)) # 343 - no observations in the dataset
sum(maxobs_sf$maxValue1000>0,na.rm=TRUE) # 48
sum(maxobs_sf$maxValue1000==0,na.rm=TRUE) # 17 have only zero observations
nrow(maxobs) # 65 monitored polygons
#View(maxobs_sf)
```


```{r message=FALSE, warning=FALSE}
plot((maxobs_sf %>% filter(nobs>0))["maxValue1000"], key.pos=4, reset=FALSE, 
     main="Maximum Alexandrium detections")
mtext("1-Jan-2018 to 26-Apr-2023", side=1, line=-0.5, cex=0.5, adj=1)#, xlim=xlim.havelock, ylim=ylim.havelock)
plot(st_geometry(coast.marlselect), col="darkgreen", add=TRUE)
plot(st_centroid(st_geometry(marlpoly_sf.trim)), pch="+", col="white", add=TRUE)
plot((maxobs_sf %>% filter(nobs>0,maxValue1000==0))["maxValue1000"], col="white", add=TRUE)
plot(st_centroid(maxobs_sf %>% filter(nobs>0,maxValue1000==0)), pch="+", col="red", add=TRUE)
```

```{r message=FALSE, warning=FALSE}
plot((maxobs_sf %>% filter(nobs>0))["maxValue1000"], key.pos=4, reset=FALSE, 
     main="Maximum Alexandrium detections: Nydia Bay", xlim=xlim.nydia, ylim=ylim.nydia)
mtext("1-Jan-2018 to 26-Apr-2023", side=1, line=-0.5, cex=0.5, adj=1)#, xlim=xlim.havelock, ylim=ylim.havelock)
plot(st_geometry(coast.marlselect), col="darkgreen", add=TRUE)
plot(st_centroid(st_geometry(marlpoly_sf.trim)), pch="+", col="white", add=TRUE)
plot((maxobs_sf %>% filter(nobs>0,maxValue1000==0))["maxValue1000"], col="white", add=TRUE)
plot(st_centroid(maxobs_sf %>% filter(nobs>0,maxValue1000==0)), pch="+", col="red", add=TRUE)
```


```{r}
wmin <- min(events.alex.week.2018$mWeek)
wmax <- max(events.alex.week.2018$mWeek)
c(wmin,wmax) # 2505-2782 (278 weeks)
wmax-wmin+1
```

```{r}
xrange <- range(marlpoly_sf.trim$volume)
breaks <- seq(from=xrange[1], to=xrange[2], length=9)
plot(marlpoly_sf.trim["volume"],
     key.pos=4, reset=FALSE,
     breaks=breaks)
```

```{r}
monitored.polygons <- maxobs$CLUSTER_ID
nmonitored <- length(monitored.polygons)
plot(NA,NA, xlim=range(events.alex.week.2018$Date), ylim=c(0,nmonitored+1), xlab="Date", ylab="", axes=FALSE)
axis.Date(1); axis(2, at=1:nmonitored, lab=monitored.polygons, las=2, cex.axis=0.5); box()
idx <- events.alex.week.2018$CLUSTER_ID%in%monitored.polygons
#points(events.alex.week.2018$mDate[idx], match(events.alex.week.2018$CLUSTER_ID[idx],monitored.polygons),
#       pch=16, cex=2*events.alex.week.2018$Value[idx]/1000/100, col="#00995555")
points(events.alex.week.2018$mDate[idx], match(events.alex.week.2018$CLUSTER_ID[idx],monitored.polygons),
       pch=16, cex=0.5*log(events.alex.week.2018$Value[idx]/1000), col="#00995555")
title("Detections by polygon over time (log scale)")
```



```{r warning=FALSE, message=FALSE}
par(opar); par(oma=c(3.1,0.5,0,0)); par(mar=1.1*c(2.5,1,4.1,4.1))
logscale <- TRUE
#logscale <- FALSE
if(logscale) {
  xrange <- log(1+range(events.alex.week.2018$Value)/1000)
} else {
  xrange <- range(events.alex.week.2018$Value)/1000
}
nbreaks <- 9
breaks <- seq(from=xrange[1], to=xrange[2], length=nbreaks)
colvec <- colorRampPalette(c("light grey","red"))(nbreaks-1)
outdir <- c("ignore/fig1/")
fstem <- "poly1"
ofile <- paste0("ignore/",fstem,".gif")
interactive <- FALSE
#interactive <- TRUE
wrange <- wmin:wmax 
#wrange <- wmin+0:10
xlim <- xlim.marlselect; ylim <- ylim.marlselect
xlim <- xlim.nydia; ylim <- ylim.nydia
#date.axis <- FALSE
date.axis <- TRUE
i <- 0
for(w in wrange) { 
  thisweek.poly <- merge(marlpoly_sf.trim, 
             events.alex.week.2018[events.alex.week.2018$mWeek==w,c("CLUSTER_ID","Value")] %>% 
               mutate(Value1000=Value/1000)) 
  if(logscale) thisweek.poly$Value1000 <- log(1+thisweek.poly$Value1000)
  i <- i+1
  fname <- sprintf("%s/%s%04d.png", outdir, fstem, i)
  if(!interactive) png(file=fname, width=480, height=480)
  par(oma=c(3.1,0.5,0,0))
  par(mar=1.1*c(2.5,1,4.1,4.1))
  plot(thisweek.poly["Value1000"], 
       pal=colvec,
       key.pos=4,
       breaks=breaks,
       reset=FALSE, 
       xlim=xlim, ylim=ylim, main="", axes=FALSE)
  plot(st_geometry(coast.marlselect), col="light green", add=TRUE)
  title(main=paste0("Week ",w))
  box()
  mtext(format(week.as.date(w), "%d-%m-%Y"), 
        side=3, line=0, adj=1, cex=0.8)
  mtext(ifelse(logscale,"Log scale","Linear scale"), 
        side=3, line=0, adj=0, cex=0.8)
  if(date.axis) {
    par(usr=c(as.numeric(week.as.date(wrange[c(1,length(wrange))])),0,1))
    axis.Date(1)
    points(week.as.date(w),0,pch=16,cex=2,xpd=TRUE)
  }
  if(!interactive) dev.off()
  #plot(st_centroid(st_geometry(marlpoly_sf.trim)), pch="+", col="white", add=TRUE)
}
par(opar)
if(!interactive) {
  image_list <- lapply(list.files(outdir,full.names=TRUE), image_read)
  poly_animated <- image_animate(image_join(image_list), fps=20)
  image_write(poly_animated, ofile)
  unlink(list.files(outdir,full.names=TRUE))
}
```

```{r}
print(poly_animated)
```

# Connectivity and Migration matrices

In the matrices below the rows are the destination locations ($i=1,\dots,n$) and the columns are the source 
locations ($j=1,\ldots,n$).

Notation: at time step $t\rightarrow t+1$ we release Mass $M_{t.j}$ from polygon $j$.  Mass $M_{tij}$ arrives in polygon $i$
from polygon $j$, thus
\[
   M_{t.j} = \sum_{i=1}^n M_{tij}
\]
The proportion of mass leaving polygon $j$ at time $t$ and arriving at polygon $i$ at time $t+1$ is
\[
   P_{tij} = \frac{M_{tij}}{M_{t.j}}
\]
with row sums
\[
   \sum_{i=1}^n P_{tij} = 1\qquad \text{for all polygons $j$ and times $t$}
\]
$P_t = (P_{tij})$ is the **connectivity matrix** at time $t$.

The total mass arriving in polygon $i$ from all sources (including itself) is 
\[
   M_{ti.} = \sum_{j=1}^n M_{tij}
\]
The proportion of mass arriving in polygon $i$ at time $t+1$ that came from polygon $j$ at time $t$ is
\[
   Q_{tij} = \frac{M_{tij}}{M_{ti.}}
\]
with column sums
\[
   \sum_{i=1}^n Q_{tij} = 1\qquad \text{for all polygons $j$ and times $t$}
\]
$Q_t = (Q_{tij})$ is the **migration matrix** at time $t$.


**Notes from Romain**

I am sending you 2 matrices with two different ways of computing the connectivity. 
Each file has one sheet per release day and each sheet shows the connectivity after 7 days of tracking. 

Rows represent the source locations and columns represent the receiving locations (ordered following the sampling stations ID). 

In Connectivity_matrices_04_to_07_2018, you will find the connectivity represented as a percentage of the 
total released at each sampling station, so each row almost add to 1 (some particles are not within a polygon 
at the end of the run, and therefore they are not counted).

In Migration_matrices_04_to_07_2018, you will find the connectivity normalized per column (sum to 1 per column). 
This gives you the percentage of the contribution of each source location to a receiving location.

**RA:** We reverse the rows/columns when reading in: connectivities ($P$) should then have unit column sums, and
migrations ($Q$) have unit row sums

Note range=cell_cols(1:408) specifies to read 408 columns - otherwise leading blank columns get dropped

```{r}
getmatrices <- TRUE
getmatrices <- FALSE
if(getmatrices) {
  # Connection and migration matrices for the polygons
  fname <- "data/connectivity_matrix.xlsx"
  nsheets <- length(excel_sheets(fname))
  conn <- lapply(1:nsheets, function(i) t(as.matrix(read_excel(fname, sheet=i, 
                                                               col_names=FALSE,
                                                               range=cell_cols(1:408)))))
  names(conn) <- excel_sheets(fname)

  #fname <- "data/Migration_matrices_04_to_07_2018.xls"
  fname <- "data/migration_matrix.xlsx"
  nsheets <- length(excel_sheets(fname))
  migr <- lapply(1:nsheets, function(i) t(as.matrix(read_excel(fname, sheet=i, 
                                                               col_names=FALSE,
                                                               range=cell_cols(1:408)))))
  names(migr) <- excel_sheets(fname)
  matdates <- as.Date(names(migr),format="%Y%m%d")
}
#save.image()
```

```{r}
sapply(conn,dim)-408
sapply(migr,dim)-408
```

Correctly identifies the columns as source polygons, rows as destinations

The trouble is that particles have only been released in 40 of the polygons: not all 408

```{r}
dim(migr[[1]])

range(apply(conn[[1]],1,sum,na.rm=TRUE)) # row sums 0 1.802315
range(apply(conn[[1]],2,sum,na.rm=TRUE)) # col sums 0 1

table(apply(conn[[1]],1,sum,na.rm=TRUE)>0) # 320 nonzero,  88 zero
table(apply(conn[[1]],2,sum,na.rm=TRUE)>0) #  40 nonzero, 368 zero
#  conn[i,j] has sum_i conn[i,j] = 1 for all j where particles have been released
#  i.e. conn[i,j] = proportion of mass from j that ends up in polygon i
#       conn[i,j] = P_{ij}

hist(apply(conn[[1]],1,sum,na.rm=TRUE))
hist(apply(conn[[1]],2,sum,na.rm=TRUE))
```

```{r}
range(apply(migr[[1]],1,sum,na.rm=TRUE)) # row sums 0 1
range(apply(migr[[1]],2,sum,na.rm=TRUE)) # col sums 0 113.2058

table(apply(migr[[1]],1,sum,na.rm=TRUE)>0) # 320 nonzero,  88 zero
table(apply(migr[[1]],2,sum,na.rm=TRUE)>0) #  40 nonzero, 368 zero
# migr[i,j] has sum_j migr[i,j] = 1 for all i where particles have been released
# i.e. migr[i,j] = proportion of mass in i that came from one of the source polygons j

hist(apply(migr[[1]],1,sum,na.rm=TRUE))
hist(apply(migr[[1]],2,sum,na.rm=TRUE))
```


Plot water residency by polygon
```{r}
diag(conn[[1]])
modelled.poly <- which(apply(conn[[1]],2,sum)>0)
notmodelled.poly <- which(apply(conn[[1]],2,sum)==0)
```

```{r}
residency.list <- as.data.frame(sapply(conn, function(pmat) diag(pmat)))
residency.list[notmodelled.poly,] <- NA
residency.list <- data.frame(CLUSTER_ID=1:nrow(residency.list), residency.list)
residency.list[1:3,]
```

```{r}
xrange <- range(residency.list[,-1])
breaks <- seq(from=0, to=1, length=9)
plot(merge(marlpoly_sf.trim, residency.list)["X20180404"],
     key.pos=4, reset=FALSE,
     breaks=breaks, main="Residency")
```

```{r}
xrange <- range(residency.list[,-1])
breaks <- seq(from=0, to=1, length=9)
plot(merge(marlpoly_sf.trim, residency.list)["X20180404"],
     key.pos=4, reset=FALSE,
     breaks=breaks, main="Residency",
     xlim=xlim.nydia, ylim=ylim.nydia)
box()
```
